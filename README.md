# LLM Compression Papers 

## Introduction
Welcome to the LLM Compression  Papers repository! This repository is dedicated to the collection and discussion of academic and industry papers focused on the compression techniques for large language models (LLMs). As the capabilities of LLMs continue to expand, so does their size and complexity. Consequently, efficient compression methods have become crucial for making these models more accessible and practical for real-world applications.  

## Survey


| Title | Introduction | Links | Conference | Year | Code | 
| ---- | ---- | ---- | ---- | ---- | ---- | 
| SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot |SparseGPT |[paper](https://arxiv.org/abs/2301.00774)| ICML| 2023| [Code]( https://github.com/IST-DASLab/sparsegptP)|  
| Efficient Large Language Models: A Survey | |[paper](https://arxiv.org/abs/2312.03863)| arxiv| 2023| [Code](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)| 
| A Survey on Deep Neural Network Pruning:Taxonomy, Comparison, Analysis, and Recommendations | |[paper](https://arxiv.org/abs/2308.06767)| arxiv| 2023|[Code](https://github.com/hrcheng1066/awesome-pruning) | 
| A Survey on Model Compression for Large Language Models | |[paper](https://arxiv.org/abs/2308.07633)| arxiv| 2023|    | 
| The Efficiency Spectrum of Large Language Models: An Algorithmic Survey | |[paper](https://arxiv.org/abs/2312.00678)| arxiv| 2023| [Code](https://github.com/tding1/Efficient-LLM-Survey)| 
|Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems  | |[paper](https://arxiv.org/abs/2312.15234)| arxiv| 2023| |  
| Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models | |[paper](https://arxiv.org/abs/2401.00625)| arxiv| 2023| [Code](https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers)|
| A Survey on Transformer Compression | |[paper](https://arxiv.org/abs/2402.05964)| arxiv| 2024|  | 


## Network Pruning

| Title | Introduction | Links | Conference | Year | Code | 
| ---- | ---- | ---- | ---- | ---- | ---- | 
| SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot |SparseGPT |[paper](https://arxiv.org/abs/2301.00774)| ICML| 2023| [Code]( https://github.com/IST-DASLab/sparsegptP)|  
| A Simple and Effective Pruning Approach for Large Language Models | Wanda|[paper](https://arxiv.org/abs/2306.11695)| arxiv| 2023| [Code](https://github.com/locuslab/wanda)|  
| LLM-Pruner: On the Structural Pruning of Large Language Models |LLM-Pruner |[paper](https://arxiv.org/abs/2305.11627)| NeurIPS| 2023| [Code](https://github.com/horseee/LLM-Pruner)| 
| The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter | |[paper](https://arxiv.org/abs/2306.03805)| NeurIPS| 2023| [Code](https://github.com/VITA-Group/essential_sparsity)| 
| Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity |Flash-LLM |[paper](https://arxiv.org/abs/2309.10285)| VLDB| 2024| [Code](https://github.com/AlibabaResearch/flash-llm)| 
|NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models  | NASH|[paper](https://arxiv.org/abs/2310.10054)| EMNLP| 2023| [Code](https://github.com/jongwooko/NASH-Pruning-Official)| 
| Pruning Large Language Models via Accuracy Predictor | |[paper](https://arxiv.org/abs/2309.09507)| arxiv| 2023| | 
| Compressing LLMs: The Truth is Rarely Pure and Never Simple | Compressing LLMs|[paper](https://arxiv.org/abs/2310.01382)| arxiv| 2023| |  
|Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity  | |[paper](https://arxiv.org/abs/2310.02277)| arxiv| 2023| [Code](https://github.com/VITA-Group/Junk_DNA_Hypothesis)|  
| Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity | |[paper](https://arxiv.org/abs/2310.05175)| arxiv| 2023| [Code](https://github.com/luuyin/OWL)| 
| Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models |Compresso |[paper](https://arxiv.org/abs/2310.05015)| arxiv| 2023| [Code](https://github.com/microsoft/Moonlit/tree/main/Compresso)| 
|Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning  |Sheared LLaMA |[paper](https://arxiv.org/abs/2310.06694)| arxiv| 2023| [Code](https://github.com/princeton-nlp/LLM-Shearing)| 
| Sparse Finetuning for Inference Acceleration of Large Language Models | |[paper](https://arxiv.org/abs/2310.06927)| arxiv| 2023| [Code](https://github.com/IST-DASLab/SparseFinetuning)|  
| ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models | |[paper](https://arxiv.org/abs/2310.04564)| arxiv| 2023| | 
| The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning | |[paper](https://arxiv.org/abs/2310.04680)| arxiv| 2023| |  
| Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs | |[paper](https://arxiv.org/abs/2310.08915)| arxiv| 2023| |  
| One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models | |[paper](https://arxiv.org/abs/2310.09499)| arxiv| 2023|| 
| LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery |LoRAShear |[paper](https://arxiv.org/abs/2310.18356)| arxiv| 2023| [Code]()| 
| Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization | |[paper](https://arxiv.org/abs/2311.01544)| arxiv| 2023| [Code](https://github.com/Aleph-Alpha/Divergent_Tokens)| 
| Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models | |[paper](https://arxiv.org/abs/2311.04902)| arxiv| 2023| [Code](https://github.com/VILA-Lab/GBLM-Pruner)| 
| Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs  | |[paper](https://arxiv.org/abs/2310.08915)| arxiv| 2023| [Code](https://github.com/zyxxmu/DSnoT)| 
| E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity  |E-Sparse |[paper](https://arxiv.org/abs/2310.15929)| arxiv| 2023|| 
| PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs | PERP|[paper](https://arxiv.org/abs/2312.15230)| arxiv| 2023| [Code](https://github.com/ZIB-IOL/PERP)| 
|Fast and Optimal Weight Update for Pruned Large Language Models  | |[paper](https://arxiv.org/abs/2401.02938)| arxiv| 2024| [Code](https://github.com/fmfi-compbio/admm-pruning)|     
|SliceGPT: Compress Large Language Models by Deleting Rows and Columns | |[paper](https://arxiv.org/abs/2401.15024)| arxiv| 2024| [Code](https://github.com/microsoft/TransformerCompression)|   
|Shortened LLaMA: A Simple Depth Pruning for Large Language Models | |[paper](https://arxiv.org/abs/2402.02834)| arxiv| 2024||   
|SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks | |[paper](https://arxiv.org/abs/2402.09025)| arxiv| 2024| [Code](https://github.com/leapingjagg-dev/SLEB)| 
| HiRE: High Recall Approximate Top-k Estimation for Efficient LLM Inference| |[paper](https://arxiv.org/abs/2402.09360)| arxiv|2024| |  
| LaCo: Large Language Model Pruning via Layer Collapse| |[paper](https://arxiv.org/abs/2402.11187)| arxiv| 2024| |  
|ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models | |[paper](https://arxiv.org/abs/2402.13516)| arxiv| 2024| [Code](https://github.com/Raincleared-Song/sparse_gpu_operator)|  
|EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs | |[paper](https://arxiv.org/abs/2402.12419)| arxiv| 2024| [Code](https://github.com/sunggo/EBFT)|  
|BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation | |[paper](https://arxiv.org/abs/2402.16880)| arxiv| 2024| [Code](https://github.com/OpenGVLab/LLMPrune-BESA)|

## Quantization  

| Title | Introduction | Links | Conference | Year | Code |  
| ---- | ---- | ---- | ---- | ---- | ---- | 
| LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale | |[paper](https://arxiv.org/abs/2208.07339)| arxiv| 2023| | 
| GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers | GPTQ|[paper](https://arxiv.org/abs/2210.17323)| ICLR| 2022| [Code](https://github.com/IST-DASLab/gptq)| 
|SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models  | SmoothQuant|[paper](https://arxiv.org/abs/2211.10438)| ICML| 2023| [Code](https://github.com/mit-han-lab/smoothquant)|  
|GPT-Zip: Deep Compression of Finetuned Large Language Models  | GPT-Zip|[paper](https://openreview.net/forum?id=hO0c2tG2xL)| ICML| 2023| | 
| SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM | |[paper](https://arxiv.org/abs/2211.10438)| ICML| 2023| [Code](https://github.com/Adlik/smoothquantplus)| 
| QLoRA: Efficient Finetuning of Quantized LLMs | QLoRA|[paper](https://arxiv.org/abs/2305.14314)| NeurIPS| 2023| [Code](https://github.com/artidoro/qlora)| 
| QuIP: 2-Bit Quantization of Large Language Models With Guarantees | QuIP|[paper](https://.org/abs/2307.13304)| NeurIPS| 2023| [Code](https://github.com/jerry-chee/QuIP)|  
|Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization  | |[paper](https://arxiv.org/abs/2305.14152)| NeurIPS| 2023| |  
| Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing | |[paper](https://arxiv.org/abs/2306.12929)| NeurIPS| 2023| [Code](https://github.com/Qualcomm-AI-research/outlier-free-transformers)|  
| LLM-FP4: 4-Bit Floating-Point Quantized Transformers | LLM-FP4|[paper](https://arxiv.org/abs/2310.16836)| EMNLP| 2023| [Code](https://github.com/nbasyl/LLM-FP4)| 
|Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization  | |[paper](https://arxiv.org/abs/2311.05161)| EMNLP| 2023| | 
| Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge |Agile-Quant |[paper](https://arxiv.org/abs/2312.05693)| AAAI| 2024| | 
| ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks | |[paper](https://arxiv.org/abs/2312.08583)| arxiv| 2023| [Code](https://github.com/microsoft/DeepSpeed)| 
| Watermarking LLMs with Weight Quantization | |[paper](https://arxiv.org/abs/2310.11237)| arxiv| 2023| [Code](https://github.com/Twilight92z/Quantize-Watermark)| 
| AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration | AWQ|[paper](https://arxiv.org/abs/2306.00978)| arxiv| 2023| [Code](https://github.com/mit-han-lab/llm-awq)| 
| RPTQ: Reorder-based Post-training Quantization for Large Language Models | RPTQ|[paper](https://arxiv.org/abs/2304.01089)| arxiv| 2023| [Code](https://github.com/hahnyuan/RPTQ4LLM)| 
| ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation | ZeroQuant-V2|[paper](https://arxiv.org/abs/2303.08302)| arxiv| 2023| | 
| SqueezeLLM: Dense-and-Sparse Quantization | SqueezeLLM|[paper](https://arxiv.org/abs/2306.07629v2)| arxiv| 2023| [Code](https://github.com/SqueezeAILab/SqueezeLLM)| 
|Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling  | |[paper](https://arxiv.org/abs/2304.09145v1)| arxiv| 2023| | 
| Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models | |[paper](https://arxiv.org/abs/2305.12356)| arxiv| 2023| | 
| LLM-QAT: Data-Free Quantization Aware Training for Large Language Models | LLM-QAT|[paper](https://arxiv.org/abs/2305.17888)| arxiv| 2023| | 
| SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression |SpQR |[paper](https://arxiv.org/abs/2306.03078)| arxiv| 2023| [Code](https://github.com/Vahe1994/SpQR)|  
| OWQ: Lessons learned from activation outliers for weight quantization in large language models |OWQ |[paper](https://arxiv.org/abs/2306.02272)| arxiv| 2023| [Code](https://github.com/xvyaward/owq)| 
|Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study  | |[paper](https://arxiv.org/abs/2307.08072)| arxiv| 2023| [Code](https://github.com/RUCAIBox/QuantizedEmpirical)| 
| ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats |ZeroQuant-FP |[paper](https://arxiv.org/abs/2307.09782)| arxiv| 2023| | 
| FPTQ: Fine-grained Post-Training Quantization for Large Language Models |FPTQ |[paper](https://arxiv.org/abs/2308.15987)| arxiv| 2023| | 
|QuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive Algorithm  |QuantEase |[paper](https://arxiv.org/abs/2309.01885)| arxiv| 2023| | 
| Norm Tweaking: High-performance Low-bit Quantization of Large Language Models | |[paper](https://arxiv.org/abs/2309.02784)| arxiv| 2023| | 
| Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs | |[paper](https://arxiv.org/abs/2309.05516)| arxiv| 2023| [Code](https://github.com/intel/neural-compressor)|  
| QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models |QA-LoRA |[paper](https://arxiv.org/abs/2309.14717)| arxiv| 2023| [Code](https://github.com/yuhuixu1993/qa-lora)| 
| ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers | ModuLoRA|[paper](https://arxiv.org/abs/2309.16119)| arxiv| 2023| | 
| Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM | |[paper](https://arxiv.org/abs/2310.04836)| arxiv| 2023|| 
|QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources  |QFT |[paper](https://arxiv.org/abs/2310.07147)| arxiv| 2023|| 
|QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models  |QLLM |[paper](https://arxiv.org/abs/2310.08041)| arxiv| 2023| |  
| LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models |LoftQ |[paper](https://arxiv.org/abs/2310.08659)| ICLR| 2024|[Code](https://github.com/yxli2123/LoftQ)| 
| TEQ: Trainable Equivalent Transformation for Quantization of LLMs | TEQ|[paper](https://arxiv.org/abs/2310.10944)| arxiv| 2023| [Code](https://github.com/intel/neural-compressor)| 
| BitNet: Scaling 1-bit Transformers for Large Language Models | |[paper](https://arxiv.org/abs/2310.11453)| arxiv| 2023| |
| Atom: Low-bit Quantization for Efficient and Accurate LLM Serving |Atom |[paper](https://arxiv.org/abs/2310.19102)| arxiv| 2023| |
| AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models | |[paper](https://arxiv.org/abs/2311.01305)| arxiv| 2023| |
| AFPQ: Asymmetric Floating Point Quantization for LLMs | |[paper](https://arxiv.org/abs/2311.01792)| arxiv| 2023| [Code](https://github.com/zhangsichengsjtu/AFPQ)| 
| A Speed Odyssey for Deployable Quantization of LLMs | |[paper](https://arxiv.org/abs/2311.09550)| arxiv| 2023| |
| LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning | LQ-LoRA|[paper](https://arxiv.org/abs/2311.12023)| arxiv| 2023| [Code](https://github.com/HanGuo97/lq-lora)|
| Enabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and Asynchronous Dequantization | |[paper](https://arxiv.org/abs/2311.16442)| arxiv| 2023| | 
| Extreme Compression of Large Language Models via Additive Quantization | AQLM|[paper](https://arxiv.org/abs/2401.06118)| arxiv| 2023| [Code](https://github.com/vahe1994/AQLM)|  
|QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models QMoE | |[paper](https://arxiv.org/abs/2310.16795)| arxiv| 2023| [Code](https://github.com/IST-DASLab/qmoe)|   
|OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models | |[paper](https://arxiv.org/abs/2308.13137)| arxiv| 2023| [Code](https://github.com/OpenGVLab/OmniQuant)| 


## Knowledge Distillation

| Title | Introduction | Links | Conference | Year | Code | 
| ---- | ---- | ---- | ---- | ---- | ---- | 
|Specializing Smaller Language Models towards Multi-Step Reasoning  | |[paper](https://arxiv.org/abs/2301.12726)| ICML| 2023| [Code](https://github.com/FranxYao/FlanT5-CoT-Specialization)| 
|Distilling Script Knowledge from Large Language Models for Constrained Language Planning  | |[paper](https://arxiv.org/abs/2305.05252)| ACL| 2023| [Code](https://github.com/siyuyuan/coscript)| 
|SCOTT: Self-Consistent Chain-of-Thought Distillation  |SCOTT |[paper](https://arxiv.org/abs/2305.01879)| ACL| 2023| [Code](https://github.com/wangpf3/consistent-CoT-distillation)| 
| DISCO: Distilling Counterfactuals with Large Language Models |DISCO |[paper](https://arxiv.org/abs/2212.10534)| ACL| 2023| [Code](https://github.com/eric11eca/disco)| 
| I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation |I2D2 |[paper](https://arxiv.org/abs/2212.09246)| ACL| 2023| | 
| Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step | |[paper](https://arxiv.org/abs/2306.14050)| ACL| 2023| | 
| GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model | GKD|[paper](https://arxiv.org/abs/2306.06629)| ACL| 2023| [Code](https://github.com/aitsc/GLMKD)| 
| Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes | |[paper](https://arxiv.org/abs/2305.02301)| ACL| 2023| [Code](https://github.com/google-research/distilling-step-by-step)| 
| Can Language Models Teach? Teacher Explanations Improve Student Performance via Theory of Mind | |[paper](https://arxiv.org/abs/2306.09299)| NeurIPS| 2023| [Code](https://github.com/swarnaHub/ExplanationIntervention)| 
| Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents | |[paper](https://arxiv.org/abs/2310.09343)| EMNLP | 2023| [Code](https://github.com/kyle8581/DialogueCoT)| 
|PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation  | PromptMix|[paper](https://arxiv.org/abs/2310.14192)| EMNLP| 2023| [Code](https://github.com/ServiceNow/PromptMix-EMNLP-2023)| 
| Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression | |[paper](https://arxiv.org/abs/2310.15594)| EMNLP| 2023| | 
| Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models | |[paper](https://arxiv.org/abs/2310.13395)| EMNLP| 2023| [Code](https://github.com/stogiannidis/OCaTS)| 
| Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data | |[paper](https://arxiv.org/abs/2312.12832)| AAAI | 2024| [Code](https://github.com/Yiwei98/TDG)| 
| LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions |LaMini-LM |[paper](https://arxiv.org/abs/2304.14402)| arxiv| 2023| [Code](https://github.com/mbzuai-nlp/LaMini-LM)| 
| Knowledge Distillation of Large Language Models | |[paper](https://arxiv.org/abs/2306.08543)| arxiv| 2023| [Code](https://github.com/microsoft/LMOps/tree/main/minillm)| 
| Teaching Small Language Models to Reason | |[paper](https://arxiv.org/abs/2212.08410)| arxiv| 2023| | 
| Large Language Model Distillation Doesn't Need a Teacher | |[paper](https://arxiv.org/abs/2305.14864)| arxiv| 2023| [Code](https://github.com/ananyahjha93/llm-distill)| 
| The False Promise of Imitating Proprietary LLMs | |[paper](https://arxiv.org/abs/2305.15717)| arxiv|| 
| Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing | |[paper](https://arxiv.org/abs/2305.16635)| arxiv| 2023| [Code](https://github.com/jaehunjung1/impossible-distillation)| 
| PaD: Program-aided Distillation Specializes Large Models in Reasoning | PaD|[paper](https://arxiv.org/abs/2305.13888)| arxiv| 2023|| 
|RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment  |RLCD |[paper](https://arxiv.org/abs/2307.12950)| arxiv| 2023|| 
| Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA | Sci-CoT|[paper](https://arxiv.org/abs/2308.04679)| arxiv| 2023|| 
| UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition | |[paper](https://arxiv.org/abs/2308.03279)| arxiv| 2023| [Code](https://github.com/universal-ner/universal-ner)| 
| Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty |BabyLlama|[paper](https://arxiv.org/abs/2308.02019)| arxiv| 2023| [Code](https://github.com/timinar/BabyLlama)| 
| DistillSpec: Improving Speculative Decoding via Knowledge Distillation | |[paper](https://arxiv.org/abs/2310.08461)| arxiv| 2023| [Code]()| 
| Zephyr: Direct Distillation of LM Alignment | |[paper](https://arxiv.org/abs/2310.16944)| arxiv| 2023| [Code]()| 
| Towards the Law of Capacity Gap in Distilling Language Models | |[paper](https://arxiv.org/abs/2311.07052)| arxiv| 2023| [Code](https://github.com/GeneZC/MiniMA)| 
| Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models | |[paper](https://arxiv.org/abs/2311.08213)| arxiv| 2023| | 
| Mixed Distillation Helps Smaller Language Model Better Reasoning | |[paper](https://arxiv.org/abs/2312.10730)| arxiv| 2023| | 

## Fusion

| Title | Introduction | Links | Conference | Year | Code | 
| ---- | ---- | ---- | ---- | ---- | ---- | 
| TinySAM: Pushing the Envelope for Efficient Segment Anything Model | |[paper](https://arxiv.org/abs/2312.13789)| arxiv| 2023| [Code](https://github.com/xinghaochen/TinySAM)| 

## References

- <https://github.com/he-y/Awesome-Pruning>
- <https://github.com/horseee/Awesome-Efficient-LLM>
